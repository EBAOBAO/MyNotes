# 工具准备

开发工具：vscode
python 解释器
数据库：mysql
数据可视化：navicat

# (0) 开始……

爬虫（网页蜘蛛、网络机器人）：模拟浏览器发送网络请求，接收请求响应，按照一定的规则自动地抓取互联网信息的程序。

爬虫可以用来进行数据采集、网页监测（可以检测产品价格变化之类的）、信息聚合、SEO优化之类的。

用python做到只需三步！！

```python
from urllib.request import urlopen # 跟url相关的库

url = "http://www.baidu.com"
resp = urlopen(url) # 打开url，得到响应

with open("te.html", mode="w", encoding="utf-8") as f:
    f.write(resp.read().decode("utf-8")) # 读取内容（以html源代码的形式），并解码
print("good!")
```

## 爬虫流程



## web请求过程剖析

首先，是客户端使用浏览器向服务器发送请求，然后服务器返回html源代码，经由浏览器编译后呈现给用户

但如果有一些附带数据的话……

1. 服务器渲染：在服务器那边直接把数据和HTML结合在一起，统一返回给浏览器
2. 客户端渲染：客户端接收到基础网页信息后，还会执行一个脚本来再次发送请求，然后客户端浏览器接收到数据后再将它嵌入网页中（页面源代码中看不到数据）

那要是像第二种的话数据不在源代码里，我们该怎么去找呢？？

**那就要熟练使用 *浏览器抓包工具* ！！**

[什么是抓包](https://blog.csdn.net/qq_40126686/article/details/106994471)

去找什么呢？

1. **url**
2. **请求方式**
3. **参数**

## http协议

协议，即计算机之间为了进行沟通而设置的协定
常见：TCP/IP, SOAP, HTTP, SMTP

http协议即超文本传输协议，用于传输超文本文件。它将一条消息分为三大块内容，无论是请求还是响应都是。

请求：

```
请求行 -> 请求方式（get, post..） url地址 协议（招呼）
 
请求头 -> 服务器要使用的附加信息（反爬、浏览器……）（问候语）

请求体 -> 请求参数（内容）
```

响应：

```
响应行 -> 协议 状态码（200: 请求成功, 404: 页面丢失, 500: 服务器报错, 302: 请求通过，但是转到另一个地址(重定向)……）

响应头 -> 客户端要使用的附加信息（秘钥、cookie……）

响应体 -> 真正内容
```

# (1) requests入门



用requests库可以进一步简化程序！！


用get方法传递参数：

```python
import requests

q = input("huhu")

url = f"https://search.bilibili.com/all?keyword={q}"

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.115"
} # 处理一个简单的反爬，UA即描述你当前的请求是通过那些设备发出的

resp = requests.get(url, headers=headers)

print(resp.text) # 拿到页面源代码
resp.close() # 记得关掉！！
```



用post方法传递参数：

```python
import requests

url = "https://fanyi.baidu.com/sug"

s = input("英文please\n")

# 发送的数据必须放在字典中，通过参数data进行传递
data = {
    "kw": s
}

# 发送post请求
resp = requests.post(url, data=data)

print(resp.json()) # 将服务器返回的内容直接处理成json (python里面是字典)
resp.close() # 记得关掉！！
```

像这样就可以爬一些客户端渲染的东西了，但是怎样爬服务器渲染的数据呢？也就是说，怎样从html文件中提取数据呢？

# (2) 数据解析

至此我们已经掌握了抓取整个网页的基本技能。但大多数情况下我们不需要整个网页的内容，怎么办呢？ -->  **数据提取！**

这里有三种数据解析方式：

1. re解析（通过正则表达式，运行速度最快的）
2. bs4解析（最简单的）
3. xpath解析（流行的）

## re解析

### 正则表达式

正则表达式（Regular Expression） 
:	一种使用表达式的方式对字符串进行匹配的语法规则

提取到的网页源代码实质上就是一个超长的字符串，想从里面提取内容，用它再合适不过了！

[在线正则表达式测试](https://tool.oschina.net/regex)

元字符：具有固定含义的特殊符号

```
.		匹配任意一个不是\n的字符
\w		字母或数字或下划线（密码、邮箱之类的有用！）
\s		匹配空白字符，比如空格，tab、换行等
\d		数字
\n		换行
\t		制表符
\b		匹配的是单词的边界

^		字符串的开始
&		字符串的结尾

\W		非字母或数字或下划线
\S		非空白符
\D		非数字

a|b		字符串a或字符串b
()		分组
[]		字符组（看一个字符在这里出没出现，出现了就匹配）（-: 范围）
[^]		非字符组

为了适应这一点，正则表达式引擎在字符组中使用**连字符`(-)`代表区间**，依照这个规则，我们可以总结出三点：

1.  要匹配任意数字可以使用`[0-9]`；
2.  如果想要匹配所有小写字母，可以写成`[a-z]`；
3.  想要匹配所有大写字母可以写成`[A-Z]`。
```

---
正则表达使用了 `-` 号代表了**区间**，但是我们有时候需要匹配的符号就是 `-`号，该怎么办呢？

这个时候我们需要对`-`号进行**转义**操作，即 `\-`。

在正则中使用 `\` 就可以进行对特殊符号进行转义，对 `-` 进行转义就可以表示为 `\-`，即 `\-` 就代表了 `-` 号本身。

有时，我们可能想要匹配一个单词的不同写法，比如`color`和`colour`，或者`honor`与`honour`。

这个时候我们可以使用 `?` 符号指定一个字符、字符组或其他基本单元可选，这意味着正则表达式引擎将会期望该字符出现**零次或一次**。

`\d{3,4}` 既可以匹配`3`个数字也可以匹配`4`个数字，不过当有`4`个数字的时候，优先匹配的是`4`个数字，这是因为正则表达式默认是**贪婪模式**，即尽可能的匹配更多字符，而要使用**非贪婪模式**，我们要在**表达式后面加上 `?`号**。

还可以使用两个速写字符指定常见的重复情况，可以使用 `+` 匹配`1`个到无数个，使用 `*`代表`0`个到无数个。

即：`+`等价于`{1,}`，`*`等价于`{0,}`。

在正则表达式中还提供了一种将表达式**分组**的机制，当使用分组时，除了获得整个匹配。还能够在匹配中选择每一个分组。

要实现分组很简单，使用`()`即可。

分组有一个非常重要的功能——`捕获数据`。所以`()`被称为捕获分组，用来捕获数据，当我们想要从匹配好的数据中提取关键数据的时候可以使用分组。

使用分组的同时还可以使用 **或者**（`or`）条件。

例如要提取所有图片文件的后缀名，可以在各个后缀名之间加上一个 `|`符号

有时候，我们并不需要捕获某个分组的内容，但是又想使用分组的特性。

这个时候就可以使用非捕获组`(?:表达式)`，从而**不捕获数据**，还能使用分组的功能。

例如想要匹配**两个字母组成的单词**或者**四个字母组成的单词**就可以使用**非捕获分组**：

那如果想让后面分组的正则和第一个分组的正则匹配同样的数据该如何做呢？

可以使用**分组的回溯引用**，使用`\N`可以引用编号为`N`的分组

很多人也称先行断言和后行断言为**环视**，也有人叫**预搜索**，其实叫什么无所谓，重要的是知道如何使用它们！

先行断言和后行断言总共有四种：

1.  正向先行断言
2.  反向先行断言
3.  正向后行断言
4.  反向后行断言

**正向先行断言：**`(?=表达式)`，指在某个位置向右看，表示所在位置右侧必须能匹配`表达式`
**反向先行断言**`(?!表达式)`的作用是保证右边不能出现某字符。

本小节只需要你记住一句话：先行断言和后行断言只有一个区别，即**先行断言从左往右看，后行断言从右往左看。**

**正向后行断言：**`(?<=表达式)`，指在某个位置向左看，表示所在位置左侧必须能匹配`表达式`

---

量词：控制**前面出现的元字符**出现的次数

```
*	重复0次或更多次
+	重复1次或更多次
?	重复0次或1次
{n}		重复n次
{n,}	重复n次或更多次
{n,m}	重复n到m次
```

```
.*?
.*
```

# 项目

## 豆瓣

进入https://movie.douban.com，使用抓包工具，发现它原来的网页上是没有任何数据的，说明它通过其他方式拿到数据

怎么办呢？可以找“网络 -> XHR”（作筛选）

```python
import requests

url = "https://movie.douban.com/j/chart/top_list?type=24&interval_id=100%3A90&action=&start=0&limit=20"
```

呃，url的参数太长了，有没有其他的封装参数的方式呢？

```python
import requests

url = "https://movie.douban.com/j/chart/top_list"

## 重新封装参数

param = {
    "type": "24",
    "interval_id": "100:90",
    "action": "",
    "start": 0,
    "limit": 20,
}

resp = requests.get(url=url, params=param)

print(resp.request.url)
```

嗯！……但是resp.text什么也没有返回呀？

*这是被反爬了……*

怎么办呢？挨个尝试到底是什么原因，**首先看一下user_agent**

**使用 `resp.request.headers`**（`request`即响应所对应的请求对象）

```python
{'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}
```

啊……

```python
import requests

url = "https://movie.douban.com/j/chart/top_list"

## 重新封装参数

param = {
    "type": "24",
    "interval_id": "100:90",
    "action": "",
    "start": 0,
    "limit": 20,
}

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 Edg/109.0.1518.115"
}

resp = requests.get(url=url, params=param, headers=headers)

print(resp.text)
```

行了。

注意原网站向下滑时，又返回了一些新数据，使用抓包工具得参数中`start`发生了变化，也就是说要是搞个循环遍历start，就能爬来很多的数据……？

## 百度翻译

首先打开百度翻译网站，使用检查（-> 网络）观察向文本框里输入内容时会有什么变化：

响应返回了名为sug的文件，查看其“预览”，正是我们想要的数据。很好！接下去就到“标头”里找访问它的url及请求方法

```
1.  请求 URL:
    https://fanyi.baidu.com/sug
2.  请求方法:
    POST
3.  状态代码:
    200 OK
4.  远程地址:
    36.155.169.254:443
5.  引用者策略:
    strict-origin-when-cross-origin
```


# 项目的思路

毕业设计（后端，AI）

## 接单流程

客户需要的结果：csv 文件或 excel。

1. 页面结构分析
2. 数据包分析